# üìå Databricks: Conceitos e Utilidades

## üîπ O que √© o Databricks?
Databricks √© uma plataforma unificada de an√°lise de dados baseada em Apache Spark, projetada para processamento massivo de dados, machine learning e an√°lise avan√ßada. Ele combina Data Engineering, Data Science e BI em um √∫nico ambiente escal√°vel e colaborativo.

## üîπ Principais Conceitos

### 1Ô∏è‚É£ **Lakehouse Architecture**
- Combina os benef√≠cios do **Data Lake** e do **Data Warehouse**.
- Permite armazenar dados brutos, processados e refinados no mesmo local.
- Usa **Delta Lake** para versionamento e otimiza√ß√£o dos dados.

### 2Ô∏è‚É£ **Delta Lake**
- Camada de armazenamento que melhora a confiabilidade dos Data Lakes.
- Oferece **ACID Transactions**, **Time Travel** e **Schema Evolution**.
- Otimiza consultas com **Z-Ordering** e **Compaction**.

### 3Ô∏è‚É£ **Clusters**
- Ambiente de computa√ß√£o escal√°vel baseado no Apache Spark.
- Pode ser configurado como **All-Purpose Clusters** (interativo) ou **Job Clusters** (automatizado).
- Suporte a inst√¢ncias **Auto-Scaling** para otimiza√ß√£o de custo.

### 4Ô∏è‚É£ **Notebooks**
- Interface colaborativa para desenvolvimento em **Python, SQL, Scala e R**.
- Suporta **visualiza√ß√µes interativas** e integra√ß√£o com bibliotecas externas.

### 5Ô∏è‚É£ **Workflows e Jobs**
- Automa√ß√£o de pipelines de ETL e Machine Learning.
- Orquestra√ß√£o de tarefas com **Triggers e Depend√™ncias**.

### 6Ô∏è‚É£ **MLflow**
- Gerenciamento completo do ciclo de vida de Machine Learning.
- Rastreia experimentos, registra modelos e facilita a implanta√ß√£o.

### 7Ô∏è‚É£ **Unity Catalog**
- Gerenciamento centralizado de governan√ßa e seguran√ßa de dados.
- Permite **controle de acessos e auditoria**.

## üîπ Utilidades e Casos de Uso
‚úÖ **Data Engineering** ‚Äì ETL massivo, processamento de Big Data.  
‚úÖ **Data Science & Machine Learning** ‚Äì Treinamento e implanta√ß√£o de modelos.  
‚úÖ **Business Intelligence** ‚Äì An√°lises avan√ßadas e gera√ß√£o de insights.  
‚úÖ **Streaming de Dados** ‚Äì Processamento de dados em tempo real com Spark Streaming.  
‚úÖ **Governan√ßa e Seguran√ßa** ‚Äì Controle de acessos e conformidade com regulamenta√ß√µes.  

## üîπ Pr√°tica

### Barra lateral

* **Workspace**: Posso criar notebooks nos Clusters
* **Catalog**: Posso criar tabelas a partir de arquivos
* **Compute**: Onde gerencio meus Clusters

### Formato Parquet

* Otimiza consultas
* Armazena em forma de colunas
* Suporta compress√£o
* Arquivo dividido em dados e metadados

### C√≥digos √∫teis

#### Arquivos gerais

**Criando DF a partir de uma tabela carregada em Catalog**:

```py
%python
df_vinhos = spark.table("vinhos")
```

**Carregando um arquivo para um dataframe**:

```py
%python 
df = spark.read.format('csv').options(header='true', inferSchema='true', delimiter=';').load('/FileStore/tables/carga/clientes_cartao.csv')
```
ou
```py
%python 
df = spark.read.load("caminho.csv", header=False, format="csv", sep=",", inferSchema=True)
```

**Ver informa√ß√µes do DF**: `df.printSchema()`

**Listar arquivos que est√£o no DBFS**: `%fs ls /caminho`

**Informa√ß√µes de um arquivo**: `%fs head /caminho`

#### JSON

**Carregando mais de um arquivo json (com mesma estrutura) para um DF**:

```py
%python
df = spark.read.json(['caminho1', 'caminho2'])
```

**Carregando todos arquivos json (com mesma estrutura) para um DF**:

```py
%python 
df = spark.read.json("caminho/*.json") 
```

**Transformar um DF em arquivo**

```py
df.write.json("/caminho")
```

**Cria√ß√£o de uma View a partir de arquivos Json e consult√°-la (melhor performance)**

```py
%python
spark.sql("CREATE OR REPLACE TEMPORARY VIEW nome_view USING json OPTIONS" +
          " (path '/caminho')")
spark.sql("select coluna from nome_view")
```

#### Parquet

**Gravando um DF em um parquet**: `df.write.parquet("/caminho")`

**Sobrescrever um parquet**: `df.write.mode('overwrite').parquet('/caminho')`

**Ler arquivo parquet e guardar em um DF**: `df.read.parquet('/caminho')`

**Consulta SQL a partir desse DF**:

```py
df.createOrReplaceTempView('view_parquet')
resultadoSQL = spark.sql("select * from view_parquet")
```
